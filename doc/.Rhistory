points(as.vector(unlist(model_values)), err_cv[,1], col = "blue", pch=16)
lines(as.vector(unlist(model_values)), err_cv[,1], col = "blue")
arrows(as.vector(unlist(model_values)), err_cv[,1] - err_cv[,2], as.vector(unlist(model_values)), err_cv[,1] + err_cv[,2],
length = 0.1, angle = 90, code = 3)
} else if(length(model_values[[1]]) == 2){
# do a heatmap
par_1_name = names(model_values[[1]])[1]
par_2_name = names(model_values[[1]])[2]
par_1 = sapply(model_values, function(x) as.numeric(x[1]))
par_2 = sapply(model_values, function(x) as.numeric(x[2]))
# construct a dataframe containing for each row the value of 1st param, the value of 2nd param, and the cv results (mean error and std of error)
err = data.frame(par_1, par_2, mean_err_cv = err_cv[,1], std_err_cv = err_cv[,2])
# plot those results in a 2d heatmap
ggplot(data = err[,c(1,2,3)], aes(x=par_1, y=par_2, fill=mean_err_cv)) +
geom_tile() +
geom_text(aes(par_1, par_2, label = round(mean_err_cv, digits=2)), color = "black", size = 4) +
# scale_fill_gradient2(low = "green", high = "red", mid = "white",
#   midpoint = 0.5, limit = c(0, 1), space = "Lab") +
labs(x=par_1_name, y=par_2_name) +
theme(
panel.grid.major = element_blank(),
panel.background = element_blank()
)
} else{
# TODO: handle visualization for 3 parameters or mroe
}
#dev.off()
}
tm_train
tm_train
colnames(tm_train)
tm_train
str(tm_train)
tm_train
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir, indexes=train_indexes,
method = "rgb", export = FALSE))
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(img_train_dir, indexes = "all",
export = FALSE))
}
save(dat_train, file = "../output/feature_train.RData")
source("../lib/cross_validation.R")
# if you're testing on a subset of the data, for debugging purposes:
# (note that you can't do cv if you don't have enough samples (less than 100) even for debbuging)
if (train_indexes != "all"){
label_train <- label_train[train_indexes]
}
# choose which model to train
model = "randomForest"
if(run.cv){
err_cv <- array(dim = c(length(model_values), 2))
err_cv
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[[k]], model, K)
print(paste0('err for this k: ', err_cv[k,]))
}
save(err_cv, file = paste0("../output/err_cv_", model, ".RData"))
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(img_train_dir, indexes = "all",
export = FALSE))
}
save(dat_train, file = "../output/feature_train.RData")
dogs_indexes <- which(label_train == 0)
cats_indexes <- which(label_train == 1)
dat_train <- dat_train[c(dogs_indexes[1:652], cats_indexes),]
label_train <- label_train[c(dogs_indexes[1:652], cats_indexes)]
source("../lib/train.R")
source("../lib/test.R")
source("../lib/cross_validation.R")
# choose which model to train
model = "randomForest"
if(run.cv){
err_cv <- array(dim = c(length(model_values), 2))
err_cv
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[[k]], model, K)
print(paste0('err for this k: ', err_cv[k,]))
}
save(err_cv, file = paste0("../output/err_cv_", model, ".RData"))
}
library(ggplot2)
if(run.cv){
# load("../output/err_cv.RData")
# pdf("../fig/cv_results.pdf", width=7, height=5)
if (length(model_values[[1]]) == 1){
# plot, like in the starter code
plot(as.vector(unlist(model_values)), err_cv[,1], xlab = "Cost", ylab = "CV Error",
main = "Cross Validation Error", type = "n", ylim = c(0, 0.35))
points(as.vector(unlist(model_values)), err_cv[,1], col = "blue", pch=16)
lines(as.vector(unlist(model_values)), err_cv[,1], col = "blue")
arrows(as.vector(unlist(model_values)), err_cv[,1] - err_cv[,2], as.vector(unlist(model_values)), err_cv[,1] + err_cv[,2],
length = 0.1, angle = 90, code = 3)
} else if(length(model_values[[1]]) == 2){
# do a heatmap
par_1_name = names(model_values[[1]])[1]
par_2_name = names(model_values[[1]])[2]
par_1 = sapply(model_values, function(x) as.numeric(x[1]))
par_2 = sapply(model_values, function(x) as.numeric(x[2]))
# construct a dataframe containing for each row the value of 1st param, the value of 2nd param, and the cv results (mean error and std of error)
err = data.frame(par_1, par_2, mean_err_cv = err_cv[,1], std_err_cv = err_cv[,2])
# plot those results in a 2d heatmap
ggplot(data = err[,c(1,2,3)], aes(x=par_1, y=par_2, fill=mean_err_cv)) +
geom_tile() +
geom_text(aes(par_1, par_2, label = round(mean_err_cv, digits=2)), color = "black", size = 4) +
# scale_fill_gradient2(low = "green", high = "red", mid = "white",
#   midpoint = 0.5, limit = c(0, 1), space = "Lab") +
labs(x=par_1_name, y=par_2_name) +
theme(
panel.grid.major = element_blank(),
panel.background = element_blank()
)
} else{
# TODO: handle visualization for 3 parameters or mroe
}
#dev.off()
}
model_best <- model_values[1]
if(run.cv){
model_best <- model_values[which.min(err_cv[, 1])]
}
# par_best <- list(ntree = model_best$ntree, mtry = model_best$mtry)
model_best[[1]]
# tm_train <- NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, model, model_best[[1]]))
tm_train
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir, indexes=train_indexes,
method = "hog", export = FALSE))
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(img_train_dir, indexes = "all",
export = FALSE))
}
save(dat_train, file = "../output/feature_train.RData")
dogs_indexes <- which(label_train == 0)
cats_indexes <- which(label_train == 1)
dat_train <- dat_train[c(dogs_indexes[1:652], cats_indexes),]
label_train <- label_train[c(dogs_indexes[1:652], cats_indexes)]
source("../lib/train.R")
source("../lib/test.R")
source("../lib/cross_validation.R")
# if you're testing on a subset of the data, for debugging purposes:
# (note that you can't do cv if you don't have enough samples (less than 100) even for debbuging)
if (train_indexes != "all"){
label_train <- label_train[train_indexes]
}
# choose which model to train
model = "randomForest"
if(run.cv){
err_cv <- array(dim = c(length(model_values), 2))
err_cv
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[[k]], model, K)
print(paste0('err for this k: ', err_cv[k,]))
}
save(err_cv, file = paste0("../output/err_cv_", model, ".RData"))
}
library(ggplot2)
if(run.cv){
# load("../output/err_cv.RData")
# pdf("../fig/cv_results.pdf", width=7, height=5)
if (length(model_values[[1]]) == 1){
# plot, like in the starter code
plot(as.vector(unlist(model_values)), err_cv[,1], xlab = "Cost", ylab = "CV Error",
main = "Cross Validation Error", type = "n", ylim = c(0, 0.35))
points(as.vector(unlist(model_values)), err_cv[,1], col = "blue", pch=16)
lines(as.vector(unlist(model_values)), err_cv[,1], col = "blue")
arrows(as.vector(unlist(model_values)), err_cv[,1] - err_cv[,2], as.vector(unlist(model_values)), err_cv[,1] + err_cv[,2],
length = 0.1, angle = 90, code = 3)
} else if(length(model_values[[1]]) == 2){
# do a heatmap
par_1_name = names(model_values[[1]])[1]
par_2_name = names(model_values[[1]])[2]
par_1 = sapply(model_values, function(x) as.numeric(x[1]))
par_2 = sapply(model_values, function(x) as.numeric(x[2]))
# construct a dataframe containing for each row the value of 1st param, the value of 2nd param, and the cv results (mean error and std of error)
err = data.frame(par_1, par_2, mean_err_cv = err_cv[,1], std_err_cv = err_cv[,2])
# plot those results in a 2d heatmap
ggplot(data = err[,c(1,2,3)], aes(x=par_1, y=par_2, fill=mean_err_cv)) +
geom_tile() +
geom_text(aes(par_1, par_2, label = round(mean_err_cv, digits=2)), color = "black", size = 4) +
# scale_fill_gradient2(low = "green", high = "red", mid = "white",
#   midpoint = 0.5, limit = c(0, 1), space = "Lab") +
labs(x=par_1_name, y=par_2_name) +
theme(
panel.grid.major = element_blank(),
panel.background = element_blank()
)
} else{
# TODO: handle visualization for 3 parameters or mroe
}
#dev.off()
}
ggsave("heatmap.png",
path = "/Users/jookim/Documents/GitHub/project-2-predictive-modelling-group-2/figs", scale = .6)
model_best <- model_values[1]
if(run.cv){
model_best <- model_values[which.min(err_cv[, 1])]
}
model_best[[1]]
packages.used=c("gbm", "knn", "xgboost", "randomForest", "ggplot2", "OpenImageR", "grDevices", "e1071", "pillar", "dplyr", "tidyr")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
library("EBImage")
library("gbm")
library("ggplot2")
experiment_dir <- "../data/pets/" # This will be modified for different data sets.
img_train_dir  <- paste(experiment_dir, "train/", sep="")
img_test_dir   <- paste(experiment_dir, "test/", sep="")
run.cv            <- TRUE # run cross-validation on the training set
K                 <- 5    # number of CV folds
run.feature.train <- TRUE # process features for training set
run.test          <- FALSE # run evaluation on an independent test set
run.feature.test  <- FALSE # process features for test set
# Model values for randomForest (two parameters: ntree and mtry)
model_values <- list()
k = 1
for(i in seq(100, 500, 100)){
for(j in seq(50, 100, 10)){
model_values[[k]] = list(ntree = i, mtry = j)
k = k + 1
}
}
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep = ""), header = F)
label_train <- as.numeric(unlist(label_train) == "cat")
# train_indexes <- seq(1,100) # set to "all" if you want to train on full dataset
train_indexes <- "all"
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir, indexes=train_indexes,
method = "sift", export = FALSE))
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(img_train_dir, indexes = "all",
export = FALSE))
}
save(dat_train, file = "../output/feature_train.RData")
source("../lib/train.R")
source("../lib/test.R")
source("../lib/cross_validation.R")
# if you're testing on a subset of the data, for debugging purposes:
# (note that you can't do cv if you don't have enough samples (less than 100) even for debbuging)
if (train_indexes != "all"){
label_train <- label_train[train_indexes]
}
# choose which model to train
model = "randomForest"
if(run.cv){
err_cv <- array(dim = c(length(model_values), 2))
err_cv
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[[k]], model, K)
print(paste0('err for this k: ', err_cv[k,]))
}
save(err_cv, file = paste0("../output/err_cv_", model, ".RData"))
}
dogs_indexes <- which(label_train == 0)
cats_indexes <- which(label_train == 1)
dat_train <- dat_train[c(dogs_indexes[1:652], cats_indexes),]
label_train <- label_train[c(dogs_indexes[1:652], cats_indexes)]
source("../lib/cross_validation.R")
# if you're testing on a subset of the data, for debugging purposes:
# (note that you can't do cv if you don't have enough samples (less than 100) even for debbuging)
if (train_indexes != "all"){
label_train <- label_train[train_indexes]
}
# choose which model to train
model = "randomForest"
if(run.cv){
err_cv <- array(dim = c(length(model_values), 2))
err_cv
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[[k]], model, K)
print(paste0('err for this k: ', err_cv[k,]))
}
save(err_cv, file = paste0("../output/err_cv_", model, ".RData"))
}
library(ggplot2)
if(run.cv){
# load("../output/err_cv.RData")
# pdf("../fig/cv_results.pdf", width=7, height=5)
if (length(model_values[[1]]) == 1){
# plot, like in the starter code
plot(as.vector(unlist(model_values)), err_cv[,1], xlab = "Cost", ylab = "CV Error",
main = "Cross Validation Error", type = "n", ylim = c(0, 0.35))
points(as.vector(unlist(model_values)), err_cv[,1], col = "blue", pch=16)
lines(as.vector(unlist(model_values)), err_cv[,1], col = "blue")
arrows(as.vector(unlist(model_values)), err_cv[,1] - err_cv[,2], as.vector(unlist(model_values)), err_cv[,1] + err_cv[,2],
length = 0.1, angle = 90, code = 3)
} else if(length(model_values[[1]]) == 2){
# do a heatmap
par_1_name = names(model_values[[1]])[1]
par_2_name = names(model_values[[1]])[2]
par_1 = sapply(model_values, function(x) as.numeric(x[1]))
par_2 = sapply(model_values, function(x) as.numeric(x[2]))
# construct a dataframe containing for each row the value of 1st param, the value of 2nd param, and the cv results (mean error and std of error)
err = data.frame(par_1, par_2, mean_err_cv = err_cv[,1], std_err_cv = err_cv[,2])
# plot those results in a 2d heatmap
ggplot(data = err[,c(1,2,3)], aes(x=par_1, y=par_2, fill=mean_err_cv)) +
geom_tile() +
geom_text(aes(par_1, par_2, label = round(mean_err_cv, digits=2)), color = "black", size = 4) +
# scale_fill_gradient2(low = "green", high = "red", mid = "white",
#   midpoint = 0.5, limit = c(0, 1), space = "Lab") +
labs(x=par_1_name, y=par_2_name) +
theme(
panel.grid.major = element_blank(),
panel.background = element_blank()
)
} else{
# TODO: handle visualization for 3 parameters or mroe
}
#dev.off()
}
library(ggplot2)
if(run.cv){
# load("../output/err_cv.RData")
# pdf("../fig/cv_results.pdf", width=7, height=5)
if (length(model_values[[1]]) == 1){
# plot, like in the starter code
plot(as.vector(unlist(model_values)), err_cv[,1], xlab = "Cost", ylab = "CV Error",
main = "Cross Validation Error", type = "n", ylim = c(0, 0.35))
points(as.vector(unlist(model_values)), err_cv[,1], col = "blue", pch=16)
lines(as.vector(unlist(model_values)), err_cv[,1], col = "blue")
arrows(as.vector(unlist(model_values)), err_cv[,1] - err_cv[,2], as.vector(unlist(model_values)), err_cv[,1] + err_cv[,2],
length = 0.1, angle = 90, code = 3)
} else if(length(model_values[[1]]) == 2){
# do a heatmap
par_1_name = names(model_values[[1]])[1]
par_2_name = names(model_values[[1]])[2]
par_1 = sapply(model_values, function(x) as.numeric(x[1]))
par_2 = sapply(model_values, function(x) as.numeric(x[2]))
# construct a dataframe containing for each row the value of 1st param, the value of 2nd param, and the cv results (mean error and std of error)
err = data.frame(par_1, par_2, mean_err_cv = err_cv[,1], std_err_cv = err_cv[,2])
# plot those results in a 2d heatmap
ggplot(data = err[,c(1,2,3)], aes(x=par_1, y=par_2, fill=mean_err_cv)) +
geom_tile() +
geom_text(aes(par_1, par_2, label = round(mean_err_cv, digits=2)), color = "black", size = 4) +
# scale_fill_gradient2(low = "green", high = "red", mid = "white",
#   midpoint = 0.5, limit = c(0, 1), space = "Lab") +
labs(x=par_1_name, y=par_2_name) +
theme(
panel.grid.major = element_blank(),
panel.background = element_blank()
)
} else{
# TODO: handle visualization for 3 parameters or mroe
}
#dev.off()
}
ggsave("heatmap_SIFT.png",
path = "/Users/jookim/Documents/GitHub/project-2-predictive-modelling-group-2/figs", scale = .6)
model_best <- model_values[1]
if(run.cv){
model_best <- model_values[which.min(err_cv[, 1])]
}
# tm_train <- NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, model, model_best[[1]]))
# save(fit_train, file = "../output/fit_train.RData")
tm_train
model_best
train_data <- list()
cropped <- list()
resized <- list()
for (i in 1:n_files){
path <- paste(img_train_dir,"pet", i, ".jpg", sep = "")
train_data[[i]] <- readImage(path)
cropped[[i]] <- if(ncol(train_data[[i]]) > nrow(train_data[[i]])){
cropImage(train_data[[i]], nrow(train_data[[i]]) - 1, nrow(train_data[[i]]) - 1,
type = "equal_spaced")
} else {
cropImage(train_data[[i]], ncol(train_data[[i]]) - 1, ncol(train_data[[i]]) - 1,
type = "equal_spaced")
}
resized[[i]] <- resizeImage(cropped[[i]], 64, 64, method = "bilinear")
}
experiment_dir <- "../data/pets/"
img_train_dir  <- paste(experiment_dir, "train/", sep = "")
hog_feature_train_dir  <- "../output/"
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep = ""), header = F)
label_train <- as.numeric(unlist(label_train) == "cat")
n_files <- length(list.files(img_train_dir))
train_data <- list()
cropped <- list()
resized <- list()
for (i in 1:n_files){
path <- paste(img_train_dir,"pet", i, ".jpg", sep = "")
train_data[[i]] <- readImage(path)
cropped[[i]] <- if(ncol(train_data[[i]]) > nrow(train_data[[i]])){
cropImage(train_data[[i]], nrow(train_data[[i]]) - 1, nrow(train_data[[i]]) - 1,
type = "equal_spaced")
} else {
cropImage(train_data[[i]], ncol(train_data[[i]]) - 1, ncol(train_data[[i]]) - 1,
type = "equal_spaced")
}
resized[[i]] <- resizeImage(cropped[[i]], 64, 64, method = "bilinear")
}
# extract hog feature (cell and orientation numbers may be changed with tuning)
hog <- matrix(NA, nrow = n_files, ncol = 200)
for (i in 1:n_files){
hog <- matrix(HOG(resized[[i]], cells = 5, orientations = 8),
nrow = n_files, ncol = 200)
}
# extract hog feature (cell and orientation numbers may be changed with tuning)
hog <- matrix(NA, nrow = n_files, ncol = 200)
for (i in 1:n_files){
hog[i, ] <- HOG(resized[[i]], cells = 5, orientations = 8)
}
train_data <- list()
cropped <- list()
resized <- list()
# extract hog feature (cell and orientation numbers may be changed with tuning)
hog <- matrix(NA, nrow = n_files, ncol = 200)
for (i in 1:n_files){
hog[i, ] <- HOG(resized[[i]], cells = 5, orientations = 8)
# output constructed features (NOT SURE WHAT THIS IS DOING)
if(export){
write.table(hog,
file = paste0("../output/hog_feature_", cell, orientation, "_",
data_name, "_", set_name, ".csv"), row.names = F,
col.names = F, sep = ",")
}
return(hog)
}
# extract hog feature (cell and orientation numbers may be changed with tuning)
hog <- matrix(NA, nrow = n_files, ncol = 200)
resized[[1]]
train_data <- list()
cropped <- list()
resized <- list()
for (i in 1:n_files){
path <- paste(img_train_dir,"pet", i, ".jpg", sep = "")
train_data[[i]] <- readImage(path)
cropped[[i]] <- if(ncol(train_data[[i]]) > nrow(train_data[[i]])){
cropImage(train_data[[i]], nrow(train_data[[i]]) - 1, nrow(train_data[[i]]) - 1,
type = "equal_spaced")
} else {
cropImage(train_data[[i]], ncol(train_data[[i]]) - 1, ncol(train_data[[i]]) - 1,
type = "equal_spaced")
}
resized[[i]] <- resizeImage(cropped[[i]], 64, 64, method = "bilinear")
}
library(OpenImageR)
train_data <- list()
cropped <- list()
resized <- list()
for (i in 1:n_files){
path <- paste(img_train_dir,"pet", i, ".jpg", sep = "")
train_data[[i]] <- readImage(path)
cropped[[i]] <- if(ncol(train_data[[i]]) > nrow(train_data[[i]])){
cropImage(train_data[[i]], nrow(train_data[[i]]) - 1, nrow(train_data[[i]]) - 1,
type = "equal_spaced")
} else {
cropImage(train_data[[i]], ncol(train_data[[i]]) - 1, ncol(train_data[[i]]) - 1,
type = "equal_spaced")
}
resized[[i]] <- resizeImage(cropped[[i]], 64, 64, method = "bilinear")
}
resized[[1]]
# extract hog feature (cell and orientation numbers may be changed with tuning)
hog <- matrix(NA, nrow = n_files, ncol = 200)
for (i in 1:n_files){
hog[i, ] <- HOG(resized[[i]], cells = 5, orientations = 8)
# output constructed features (NOT SURE WHAT THIS IS DOING)
if(export){
write.table(hog,
file = paste0("../output/hog_feature_", cell, orientation, "_",
data_name, "_", set_name, ".csv"), row.names = F,
col.names = F, sep = ",")
}
return(hog)
}
for (i in 1:n_files){
for (i in 1:n_files){
hog[i, ] <- HOG(resized[[i]], cells = 5, orientations = 8)}
dim(hog)
# extract hog feature (cell and orientation numbers may be changed with tuning)
hog <- matrix(NA, nrow = n_files, ncol = 200)
for (i in 1:n_files){
hog[i, ] <- HOG(resized[[i]], cells = 5, orientations = 8)
}
hog <- matrix(NA, nrow = n_files, ncol = 200)
for (i in 1:n_files){
hog[i, ] <- HOG(resized[[i]], cells = 5, orientations = 8)
}
}
